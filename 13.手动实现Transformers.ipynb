{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1443aa2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_x= {'<SOS>': 0, '<EOS>': 1, '<PAD>': 2, '0': 3, '1': 4, '2': 5, '3': 6, '4': 7, '5': 8, '6': 9, '7': 10, '8': 11, '9': 12, 'q': 13, 'w': 14, 'e': 15, 'r': 16, 't': 17, 'y': 18, 'u': 19, 'i': 20, 'o': 21, 'p': 22, 'a': 23, 's': 24, 'd': 25, 'f': 26, 'g': 27, 'h': 28, 'j': 29, 'k': 30, 'l': 31, 'z': 32, 'x': 33, 'c': 34, 'v': 35, 'b': 36, 'n': 37, 'm': 38}\n",
      "vocab_y= {'<SOS>': 0, '<EOS>': 1, '<PAD>': 2, '0': 3, '1': 4, '2': 5, '3': 6, '4': 7, '5': 8, '6': 9, '7': 10, '8': 11, '9': 12, 'Q': 13, 'W': 14, 'E': 15, 'R': 16, 'T': 17, 'Y': 18, 'U': 19, 'I': 20, 'O': 21, 'P': 22, 'A': 23, 'S': 24, 'D': 25, 'F': 26, 'G': 27, 'H': 28, 'J': 29, 'K': 30, 'L': 31, 'Z': 32, 'X': 33, 'C': 34, 'V': 35, 'B': 36, 'N': 37, 'M': 38}\n"
     ]
    }
   ],
   "source": [
    "#第13章/定义字典\n",
    "vocab_x = '<SOS>,<EOS>,<PAD>,0,1,2,3,4,5,6,7,8,9,q,w,e,r,t,y,u,i,o,p,a,s,d,f,g,h,j,k,l,z,x,c,v,b,n,m'\n",
    "vocab_x = {word: i for i, word in enumerate(vocab_x.split(','))}\n",
    "\n",
    "vocab_xr = [k for k, v in vocab_x.items()]\n",
    "\n",
    "vocab_y = {k.upper(): v for k, v in vocab_x.items()}\n",
    "\n",
    "vocab_yr = [k for k, v in vocab_y.items()]\n",
    "\n",
    "print('vocab_x=', vocab_x)\n",
    "print('vocab_y=', vocab_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cee56575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0, 20, 38, 18, 23, 35, 38, 19, 33, 33, 25, 34, 34, 32, 27, 19, 32, 20,\n",
       "         38, 38, 36, 38, 26, 28, 33, 34, 33, 38,  5, 16, 30, 30, 35, 33, 25, 22,\n",
       "         38, 21, 35, 29, 26, 10, 33, 34, 11, 12, 32,  1,  2,  2]),\n",
       " tensor([ 0, 32, 32,  3,  4, 34, 33,  5, 26, 29, 35, 21, 38, 22, 25, 33, 35, 30,\n",
       "         30, 16, 10, 38, 33, 34, 33, 28, 26, 38, 36, 38, 38, 20, 32, 19, 27, 32,\n",
       "         34, 34, 25, 33, 33, 19, 38, 35, 23, 18, 38, 20,  1,  2,  2]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第13章/定义生成数据函数\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    #定义词集合\n",
    "    words = [\n",
    "        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'q', 'w', 'e', 'r',\n",
    "        't', 'y', 'u', 'i', 'o', 'p', 'a', 's', 'd', 'f', 'g', 'h', 'j', 'k',\n",
    "        'l', 'z', 'x', 'c', 'v', 'b', 'n', 'm'\n",
    "    ]\n",
    "\n",
    "    #定义每个词被选中的概率\n",
    "    p = np.array([\n",
    "        1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n",
    "        13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26\n",
    "    ])\n",
    "    p = p / p.sum()\n",
    "\n",
    "    #随机选n个词\n",
    "    n = random.randint(30, 48)\n",
    "    x = np.random.choice(words, size=n, replace=True, p=p)\n",
    "\n",
    "    #采样的结果就是x\n",
    "    x = x.tolist()\n",
    "\n",
    "    #y是对x的变换得到的\n",
    "    #字母大写,数字取9以内的互补数\n",
    "    def f(i):\n",
    "        i = i.upper()\n",
    "        if not i.isdigit():\n",
    "            return i\n",
    "        i = 9 - int(i)\n",
    "        return str(i)\n",
    "\n",
    "    y = [f(i) for i in x]\n",
    "    #逆序\n",
    "    y = y[::-1]\n",
    "\n",
    "    #y中的首字母双写\n",
    "    y = [y[0]] + y\n",
    "\n",
    "    #加上首尾符号\n",
    "    x = ['<SOS>'] + x + ['<EOS>']\n",
    "    y = ['<SOS>'] + y + ['<EOS>']\n",
    "\n",
    "    #补pad到固定长度\n",
    "    x = x + ['<PAD>'] * 50\n",
    "    y = y + ['<PAD>'] * 51\n",
    "    x = x[:50]\n",
    "    y = y[:51]\n",
    "\n",
    "    #编码成数据\n",
    "    x = [vocab_x[i] for i in x]\n",
    "    y = [vocab_y[i] for i in y]\n",
    "\n",
    "    #转tensor\n",
    "    x = torch.LongTensor(x)\n",
    "    y = torch.LongTensor(y)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b239c926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  7,  7, 11,  9,  8,  9,  8, 11,  4,  5, 11, 10, 11, 10,  9, 23,  8,\n",
       "          7,  6, 12,  5,  8,  8,  4,  3,  8, 11,  1,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2]),\n",
       " tensor([ 0,  7,  7, 11, 10,  4,  3, 12, 10,  6, 11,  6, 11, 12,  6,  7,  1,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第13章/两数相加测试\n",
    "#使用这份数据请把训练次数改为10\n",
    "def get_data():\n",
    "    #定义词集合\n",
    "    words = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "    #定义每个词被选中的概率\n",
    "    p = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "    p = p / p.sum()\n",
    "\n",
    "    #随机选n个词\n",
    "    n = random.randint(10, 20)\n",
    "    s1 = np.random.choice(words, size=n, replace=True, p=p)\n",
    "\n",
    "    #采样的结果就是s1\n",
    "    s1 = s1.tolist()\n",
    "\n",
    "    #同样的方法,再采出s2\n",
    "    n = random.randint(10, 20)\n",
    "    s2 = np.random.choice(words, size=n, replace=True, p=p)\n",
    "    s2 = s2.tolist()\n",
    "\n",
    "    #y等于s1和s2数值上的相加\n",
    "    y = int(''.join(s1)) + int(''.join(s2))\n",
    "    y = list(str(y))\n",
    "\n",
    "    #x等于s1和s2字符上的相加\n",
    "    x = s1 + ['a'] + s2\n",
    "\n",
    "    #加上首尾符号\n",
    "    x = ['<SOS>'] + x + ['<EOS>']\n",
    "    y = ['<SOS>'] + y + ['<EOS>']\n",
    "\n",
    "    #补pad到固定长度\n",
    "    x = x + ['<PAD>'] * 50\n",
    "    y = y + ['<PAD>'] * 51\n",
    "    x = x[:50]\n",
    "    y = y[:51]\n",
    "\n",
    "    #编码成数据\n",
    "    x = [vocab_x[i] for i in x]\n",
    "    y = [vocab_y[i] for i in y]\n",
    "\n",
    "    #转tensor\n",
    "    x = torch.LongTensor(x)\n",
    "    y = torch.LongTensor(y)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99e7c73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 50]), torch.Size([8, 51]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第13章/定义数据集和加载器\n",
    "#定义数据集\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        super(Dataset, self).__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return 100000\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return get_data()\n",
    "\n",
    "\n",
    "#数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=Dataset(),\n",
    "                                     batch_size=8,\n",
    "                                     drop_last=True,\n",
    "                                     shuffle=True,\n",
    "                                     collate_fn=None)\n",
    "\n",
    "#查看数据样例\n",
    "for i, (x, y) in enumerate(loader):\n",
    "    break\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ae1b754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          ...,\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True]]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第13章/定义mask_pad函数\n",
    "def mask_pad(data):\n",
    "    #b句话,每句话50个词,这里是还没embed的\n",
    "    #data = [b, 50]\n",
    "    #判断每个词是不是<PAD>\n",
    "    mask = data == vocab_x['<PAD>']\n",
    "\n",
    "    #[b, 50] -> [b, 1, 1, 50]\n",
    "    mask = mask.reshape(-1, 1, 1, 50)\n",
    "\n",
    "    #在计算注意力时,是计算50个词和50个词相互之间的注意力,所以是个50*50的矩阵\n",
    "    #是pad的列是true,意味着任何词对pad的注意力都是0\n",
    "    #但是pad本身对其他词的注意力并不是0\n",
    "    #所以是pad的行不是true\n",
    "\n",
    "    #复制n次\n",
    "    #[b, 1, 1, 50] -> [b, 1, 50, 50]\n",
    "    mask = mask.expand(-1, 1, 50, 50)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "mask_pad(x[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc80f8eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[False,  True,  True,  ...,  True,  True,  True],\n",
       "          [False, False,  True,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          ...,\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True]]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第13章/定义mask_tril函数\n",
    "def mask_tril(data):\n",
    "    #b句话,每句话50个词,这里是还没embed的\n",
    "    #data = [b, 50]\n",
    "\n",
    "    #50*50的矩阵表示每个词对其他词是否可见\n",
    "    #上三角矩阵,不包括对角线,意味着,对每个词而言,他只能看到他自己,和他之前的词,而看不到之后的词\n",
    "    #[1, 50, 50]\n",
    "    \"\"\"\n",
    "    [[0, 1, 1, 1, 1],\n",
    "     [0, 0, 1, 1, 1],\n",
    "     [0, 0, 0, 1, 1],\n",
    "     [0, 0, 0, 0, 1],\n",
    "     [0, 0, 0, 0, 0]]\"\"\"\n",
    "    tril = 1 - torch.tril(torch.ones(1, 50, 50, dtype=torch.long))\n",
    "\n",
    "    #判断y当中每个词是不是pad,如果是pad则不可见\n",
    "    #[b, 50]\n",
    "    mask = data == vocab_y['<PAD>']\n",
    "\n",
    "    #变形+转型,为了之后的计算\n",
    "    #[b, 1, 50]\n",
    "    mask = mask.unsqueeze(1).long()\n",
    "\n",
    "    #mask和tril求并集\n",
    "    #[b, 1, 50] + [1, 50, 50] -> [b, 50, 50]\n",
    "    mask = mask + tril\n",
    "\n",
    "    #转布尔型\n",
    "    mask = mask > 0\n",
    "\n",
    "    #转布尔型,增加一个维度,便于后续的计算\n",
    "    mask = (mask == 1).unsqueeze(dim=1)\n",
    "\n",
    "    return mask\n",
    "\n",
    "mask_tril(x[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a59b48af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 50, 32])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第13章/定义注意力计算函数\n",
    "def attention(Q, K, V, mask):\n",
    "    #b句话,每句话50个词,每个词编码成32维向量,4个头,每个头分到8维向量\n",
    "    #Q、K、V = [b, 4, 50, 8]\n",
    "\n",
    "    #[b, 4, 50, 8] * [b, 4, 8, 50] -> [b, 4, 50, 50]\n",
    "    #Q、K矩阵相乘,求每个词相对其他所有词的注意力\n",
    "    score = torch.matmul(Q, K.permute(0, 1, 3, 2))\n",
    "\n",
    "    #除以每个头维数的平方根,做数值缩放\n",
    "    score /= 8**0.5\n",
    "\n",
    "    #mask遮盖,mask是true的地方都被替换成-inf,这样在计算softmax的时候,-inf会被压缩到0\n",
    "    #mask = [b, 1, 50, 50]\n",
    "    score = score.masked_fill_(mask, -float('inf'))\n",
    "    score = torch.softmax(score, dim=-1)\n",
    "\n",
    "    #以注意力分数乘以V,得到最终的注意力结果\n",
    "    #[b, 4, 50, 50] * [b, 4, 50, 8] -> [b, 4, 50, 8]\n",
    "    score = torch.matmul(score, V)\n",
    "\n",
    "    #每个头计算的结果合一\n",
    "    #[b, 4, 50, 8] -> [b, 50, 32]\n",
    "    score = score.permute(0, 2, 1, 3).reshape(-1, 50, 32)\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "attention(torch.randn(8, 4, 50, 8), torch.randn(8, 4, 50, 8),\n",
    "          torch.randn(8, 4, 50, 8), torch.zeros(8, 1, 50, 50)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d308d219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.1761, -1.0523, -0.9285, -0.8047],\n",
      "         [-1.1761, -1.0523, -0.9285, -0.8047],\n",
      "         [-1.1761, -1.0523, -0.9285, -0.8047],\n",
      "         [-1.1761, -1.0523, -0.9285, -0.8047]],\n",
      "\n",
      "        [[ 0.8047,  0.9285,  1.0523,  1.1761],\n",
      "         [ 0.8047,  0.9285,  1.0523,  1.1761],\n",
      "         [ 0.8047,  0.9285,  1.0523,  1.1761],\n",
      "         [ 0.8047,  0.9285,  1.0523,  1.1761]]],\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n",
      "tensor([[[-1.3416, -0.4472,  0.4472,  1.3416],\n",
      "         [-1.3416, -0.4472,  0.4472,  1.3416],\n",
      "         [-1.3416, -0.4472,  0.4472,  1.3416],\n",
      "         [-1.3416, -0.4472,  0.4472,  1.3416]],\n",
      "\n",
      "        [[-1.3416, -0.4472,  0.4472,  1.3416],\n",
      "         [-1.3416, -0.4472,  0.4472,  1.3416],\n",
      "         [-1.3416, -0.4472,  0.4472,  1.3416],\n",
      "         [-1.3416, -0.4472,  0.4472,  1.3416]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#第13章/BatchNorm1d和LayerNorm的对比\n",
    "#标准化之后,均值是0,标准差是1\n",
    "#BN是取不同样本做标准化\n",
    "#LN是取不同通道做标准化\n",
    "#affine=True,elementwise_affine=True,指定标准化后,再计算一个线性映射\n",
    "norm = torch.nn.BatchNorm1d(num_features=4, affine=True)\n",
    "print(norm(torch.arange(32, dtype=torch.float32).reshape(2, 4, 4)))\n",
    "\n",
    "norm = torch.nn.LayerNorm(normalized_shape=4, elementwise_affine=True)\n",
    "print(norm(torch.arange(32, dtype=torch.float32).reshape(2, 4, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a6b55f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 50, 32])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第13章/多头注意力计算层\n",
    "class MultiHead(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc_Q = torch.nn.Linear(32, 32)\n",
    "        self.fc_K = torch.nn.Linear(32, 32)\n",
    "        self.fc_V = torch.nn.Linear(32, 32)\n",
    "\n",
    "        self.out_fc = torch.nn.Linear(32, 32)\n",
    "\n",
    "        self.norm = torch.nn.LayerNorm(normalized_shape=32,\n",
    "                                       elementwise_affine=True)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, Q, K, V, mask):\n",
    "        #b句话,每句话50个词,每个词编码成32维向量\n",
    "        #Q、K、V = [b, 50, 32]\n",
    "        b = Q.shape[0]\n",
    "\n",
    "        #保留下原始的Q,后面要做短接用\n",
    "        clone_Q = Q.clone()\n",
    "\n",
    "        #标准化\n",
    "        Q = self.norm(Q)\n",
    "        K = self.norm(K)\n",
    "        V = self.norm(V)\n",
    "\n",
    "        #线性运算,维度不变\n",
    "        #[b, 50, 32] -> [b, 50, 32]\n",
    "        K = self.fc_K(K)\n",
    "        V = self.fc_V(V)\n",
    "        Q = self.fc_Q(Q)\n",
    "\n",
    "        #拆分成多个头\n",
    "        #b句话,每句话50个词,每个词编码成32维向量,4个头,每个头分到8维向量\n",
    "        #[b, 50, 32] -> [b, 4, 50, 8]\n",
    "        Q = Q.reshape(b, 50, 4, 8).permute(0, 2, 1, 3)\n",
    "        K = K.reshape(b, 50, 4, 8).permute(0, 2, 1, 3)\n",
    "        V = V.reshape(b, 50, 4, 8).permute(0, 2, 1, 3)\n",
    "\n",
    "        #计算注意力\n",
    "        #[b, 4, 50, 8] -> [b, 50, 32]\n",
    "        score = attention(Q, K, V, mask)\n",
    "\n",
    "        #计算输出,维度不变\n",
    "        #[b, 50, 32] -> [b, 50, 32]\n",
    "        score = self.dropout(self.out_fc(score))\n",
    "\n",
    "        #短接\n",
    "        score = clone_Q + score\n",
    "        return score\n",
    "\n",
    "\n",
    "MultiHead()(torch.randn(8, 50, 32), torch.randn(8, 50, 32),\n",
    "            torch.randn(8, 50, 32), torch.zeros(8, 1, 50, 50)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ade4476e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 50, 32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第13章/定义位置编码层\n",
    "import math\n",
    "\n",
    "\n",
    "class PositionEmbedding(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #pos是第几个词,i是第几个维度,d_model是维度总数\n",
    "        def get_pe(pos, i, d_model):\n",
    "            d = 1e4**(i / d_model)\n",
    "            pe = pos / d\n",
    "\n",
    "            if i % 2 == 0:\n",
    "                return math.sin(pe)\n",
    "            return math.cos(pe)\n",
    "\n",
    "        #初始化位置编码矩阵\n",
    "        pe = torch.empty(50, 32)\n",
    "        for i in range(50):\n",
    "            for j in range(32):\n",
    "                pe[i, j] = get_pe(i, j, 32)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        #定义为不更新的常量\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "        #词编码层\n",
    "        self.embed = torch.nn.Embedding(39, 32)\n",
    "        #初始化参数\n",
    "        self.embed.weight.data.normal_(0, 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #[8, 50] -> [8, 50, 32]\n",
    "        embed = self.embed(x)\n",
    "\n",
    "        #词编码和位置编码相加\n",
    "        #[8, 50, 32] + [1, 50, 32] -> [8, 50, 32]\n",
    "        embed = embed + self.pe\n",
    "        return embed\n",
    "\n",
    "\n",
    "PositionEmbedding()(torch.ones(8, 50).long()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5892664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 50, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第13章/定义全连接输出层\n",
    "class FullyConnectedOutput(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=32, out_features=64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=64, out_features=32),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "        )\n",
    "\n",
    "        self.norm = torch.nn.LayerNorm(normalized_shape=32,\n",
    "                                       elementwise_affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #保留下原始的x,后面要做短接用\n",
    "        clone_x = x.clone()\n",
    "\n",
    "        #标准化\n",
    "        x = self.norm(x)\n",
    "\n",
    "        #线性全连接运算\n",
    "        #[b, 50, 32] -> [b, 50, 32]\n",
    "        out = self.fc(x)\n",
    "\n",
    "        #做短接\n",
    "        out = clone_x + out\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "FullyConnectedOutput()(torch.randn(8, 50, 32)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86d18572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 50, 32])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第13章/定义编码器\n",
    "#编码器层\n",
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mh = MultiHead()\n",
    "        self.fc = FullyConnectedOutput()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        #计算自注意力,维度不变\n",
    "        #[b, 50, 32] -> [b, 50, 32]\n",
    "        score = self.mh(x, x, x, mask)\n",
    "\n",
    "        #全连接输出,维度不变\n",
    "        #[b, 50, 32] -> [b, 50, 32]\n",
    "        out = self.fc(score)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "#编码器\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = EncoderLayer()\n",
    "        self.layer_2 = EncoderLayer()\n",
    "        self.layer_3 = EncoderLayer()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.layer_1(x, mask)\n",
    "        x = self.layer_2(x, mask)\n",
    "        x = self.layer_3(x, mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "Encoder()(torch.randn(8, 50, 32), torch.ones(8, 1, 50, 50)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6553c927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 50, 32])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第13章/定义解码器\n",
    "#解码器层\n",
    "class DecoderLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mh1 = MultiHead()\n",
    "        self.mh2 = MultiHead()\n",
    "\n",
    "        self.fc = FullyConnectedOutput()\n",
    "\n",
    "    def forward(self, x, y, mask_pad_x, mask_tril_y):\n",
    "        #先计算y的自注意力,维度不变\n",
    "        #[b, 50, 32] -> [b, 50, 32]\n",
    "        y = self.mh1(y, y, y, mask_tril_y)\n",
    "\n",
    "        #结合x和y的注意力计算,维度不变\n",
    "        #[b, 50, 32],[b, 50, 32] -> [b, 50, 32]\n",
    "        y = self.mh2(y, x, x, mask_pad_x)\n",
    "\n",
    "        #全连接输出,维度不变\n",
    "        #[b, 50, 32] -> [b, 50, 32]\n",
    "        y = self.fc(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "#解码器\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_1 = DecoderLayer()\n",
    "        self.layer_2 = DecoderLayer()\n",
    "        self.layer_3 = DecoderLayer()\n",
    "\n",
    "    def forward(self, x, y, mask_pad_x, mask_tril_y):\n",
    "        y = self.layer_1(x, y, mask_pad_x, mask_tril_y)\n",
    "        y = self.layer_2(x, y, mask_pad_x, mask_tril_y)\n",
    "        y = self.layer_3(x, y, mask_pad_x, mask_tril_y)\n",
    "        return y\n",
    "\n",
    "\n",
    "Decoder()(torch.randn(8, 50, 32), torch.randn(8, 50, 32),\n",
    "          torch.ones(8, 1, 50, 50), torch.ones(8, 1, 50, 50)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b51b9f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 50, 39])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第13章/定义主模型\n",
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed_x = PositionEmbedding()\n",
    "        self.embed_y = PositionEmbedding()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.fc_out = torch.nn.Linear(32, 39)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        #[b, 1, 50, 50]\n",
    "        mask_pad_x = mask_pad(x)\n",
    "        mask_tril_y = mask_tril(y)\n",
    "\n",
    "        #编码,添加位置信息\n",
    "        #x = [b, 50] -> [b, 50, 32]\n",
    "        #y = [b, 50] -> [b, 50, 32]\n",
    "        x, y = self.embed_x(x), self.embed_y(y)\n",
    "\n",
    "        #编码层计算\n",
    "        #[b, 50, 32] -> [b, 50, 32]\n",
    "        x = self.encoder(x, mask_pad_x)\n",
    "\n",
    "        #解码层计算\n",
    "        #[b, 50, 32],[b, 50, 32] -> [b, 50, 32]\n",
    "        y = self.decoder(x, y, mask_pad_x, mask_tril_y)\n",
    "\n",
    "        #全连接输出,维度不变\n",
    "        #[b, 50, 32] -> [b, 50, 39]\n",
    "        y = self.fc_out(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "model = Transformer()\n",
    "\n",
    "model(torch.ones(8, 50).long(), torch.ones(8, 50).long()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ff7f830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 18, 18, 11, 11,  0,  0,  0,  7,  0, 26,  0,  0,  0, 21, 19, 19, 19,\n",
       "         19, 19, 19, 21, 21,  0,  0,  0, 21, 21, 21, 21, 19, 19, 19, 21, 21, 21,\n",
       "         21, 21, 21, 21, 21, 21, 16, 16, 19, 19, 19, 21, 21, 21]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第13章/定义预测函数\n",
    "def predict(x):\n",
    "    #x = [1, 50]\n",
    "    model.eval()\n",
    "\n",
    "    #[1, 1, 50, 50]\n",
    "    mask_pad_x = mask_pad(x)\n",
    "\n",
    "    #初始化输出,这个是固定值\n",
    "    #[1, 50]\n",
    "    #[[0,2,2,2...]]\n",
    "    target = [vocab_y['<SOS>']] + [vocab_y['<PAD>']] * 49\n",
    "    target = torch.LongTensor(target).unsqueeze(0)\n",
    "\n",
    "    #x编码,添加位置信息\n",
    "    #[1, 50] -> [1, 50, 32]\n",
    "    x = model.embed_x(x)\n",
    "\n",
    "    #编码层计算,维度不变\n",
    "    #[1, 50, 32] -> [1, 50, 32]\n",
    "    x = model.encoder(x, mask_pad_x)\n",
    "\n",
    "    #遍历生成第1个词到第49个词\n",
    "    for i in range(49):\n",
    "        #[1, 50]\n",
    "        y = target\n",
    "\n",
    "        #[1, 1, 50, 50]\n",
    "        mask_tril_y = mask_tril(y)\n",
    "\n",
    "        #y编码,添加位置信息\n",
    "        #[1, 50] -> [1, 50, 32]\n",
    "        y = model.embed_y(y)\n",
    "\n",
    "        #解码层计算,维度不变\n",
    "        #[1, 50, 32],[1, 50, 32] -> [1, 50, 32]\n",
    "        y = model.decoder(x, y, mask_pad_x, mask_tril_y)\n",
    "\n",
    "        #全连接输出,39分类\n",
    "        #[1, 50, 32] -> [1, 50, 39]\n",
    "        out = model.fc_out(y)\n",
    "\n",
    "        #取出当前词的输出\n",
    "        #[1, 50, 39] -> [1, 39]\n",
    "        out = out[:, i, :]\n",
    "\n",
    "        #取出分类结果\n",
    "        #[1, 39] -> [1]\n",
    "        out = out.argmax(dim=1).detach()\n",
    "\n",
    "        #以当前词预测下一个词,填到结果中\n",
    "        target[:, i + 1] = out\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "predict(torch.ones(1, 50).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01693ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0.002 4.017234802246094 0.0\n",
      "0 2000 0.002 2.1498022079467773 0.1347517730496454\n",
      "0 4000 0.002 2.132599353790283 0.176056338028169\n",
      "0 6000 0.002 2.1317319869995117 0.20588235294117646\n",
      "0 8000 0.002 2.0808982849121094 0.20422535211267606\n",
      "0 10000 0.002 2.078150510787964 0.17687074829931973\n",
      "0 12000 0.002 1.7938580513000488 0.3333333333333333\n",
      "1 0 0.002 1.787221908569336 0.3237410071942446\n",
      "1 2000 0.002 1.8628085851669312 0.25\n",
      "1 4000 0.002 1.7083605527877808 0.3404255319148936\n",
      "1 6000 0.002 1.720707893371582 0.3561643835616438\n",
      "1 8000 0.002 1.3579456806182861 0.47619047619047616\n",
      "1 10000 0.002 1.0340052843093872 0.5833333333333334\n",
      "1 12000 0.002 1.405091404914856 0.5771812080536913\n",
      "2 0 0.002 1.172294020652771 0.6\n",
      "2 2000 0.002 0.7125112414360046 0.7202797202797203\n",
      "2 4000 0.002 1.0112520456314087 0.6369426751592356\n",
      "2 6000 0.002 0.7250597476959229 0.7448275862068966\n",
      "2 8000 0.002 0.8674978613853455 0.7404580152671756\n",
      "2 10000 0.002 1.1881616115570068 0.65\n",
      "2 12000 0.002 0.4675864279270172 0.8266666666666667\n",
      "3 0 0.001 0.8303089737892151 0.75\n",
      "3 2000 0.001 0.2294255495071411 0.9197080291970803\n",
      "3 4000 0.001 0.08018513768911362 0.9731543624161074\n",
      "3 6000 0.001 0.8068006634712219 0.8439716312056738\n",
      "3 8000 0.001 0.4353960454463959 0.8543046357615894\n",
      "3 10000 0.001 0.23123426735401154 0.9424460431654677\n",
      "3 12000 0.001 0.13621044158935547 0.9225806451612903\n",
      "4 0 0.001 0.7422864437103271 0.7834394904458599\n",
      "4 2000 0.001 0.1658288985490799 0.9548872180451128\n",
      "4 4000 0.001 0.2069556713104248 0.9527027027027027\n",
      "4 6000 0.001 0.08378917723894119 0.9662162162162162\n",
      "4 8000 0.001 0.1329379826784134 0.9609375\n",
      "4 10000 0.001 0.18102450668811798 0.9236111111111112\n",
      "4 12000 0.001 0.19995197653770447 0.9020979020979021\n",
      "5 0 0.001 0.21403124928474426 0.9027777777777778\n",
      "5 2000 0.001 0.11801331490278244 0.9586206896551724\n",
      "5 4000 0.001 0.13793347775936127 0.9455782312925171\n",
      "5 6000 0.001 0.10863324999809265 0.959349593495935\n",
      "5 8000 0.001 0.28970658779144287 0.9154929577464789\n",
      "5 10000 0.001 0.1117856502532959 0.96\n",
      "5 12000 0.001 0.1242571473121643 0.9432624113475178\n",
      "6 0 0.0005 0.3172769546508789 0.9440559440559441\n",
      "6 2000 0.0005 0.16547764837741852 0.9305555555555556\n",
      "6 4000 0.0005 0.12586116790771484 0.95\n",
      "6 6000 0.0005 0.04157395288348198 0.993006993006993\n",
      "6 8000 0.0005 0.29622605443000793 0.8789808917197452\n",
      "6 10000 0.0005 0.13073742389678955 0.9402985074626866\n",
      "6 12000 0.0005 0.06938593089580536 0.965034965034965\n",
      "7 0 0.0005 0.06543045490980148 0.9788732394366197\n",
      "7 2000 0.0005 0.06567972898483276 0.9716312056737588\n",
      "7 4000 0.0005 0.07612716406583786 0.9733333333333334\n",
      "7 6000 0.0005 0.06971929222345352 0.9795918367346939\n",
      "7 8000 0.0005 0.03161866217851639 0.9924812030075187\n",
      "7 10000 0.0005 0.05850832909345627 0.9731543624161074\n",
      "7 12000 0.0005 0.08066719770431519 0.9664429530201343\n",
      "8 0 0.0005 0.04522106423974037 0.993103448275862\n",
      "8 2000 0.0005 0.1013432964682579 0.9712230215827338\n",
      "8 4000 0.0005 0.07277103513479233 0.9645390070921985\n",
      "8 6000 0.0005 0.29492396116256714 0.8913043478260869\n",
      "8 8000 0.0005 0.04505931958556175 0.9774436090225563\n",
      "8 10000 0.0005 0.13400757312774658 0.9436619718309859\n",
      "8 12000 0.0005 0.06075035408139229 0.9801324503311258\n",
      "9 0 0.00025 0.07825443148612976 0.9791666666666666\n",
      "9 2000 0.00025 0.05356918275356293 0.9801324503311258\n",
      "9 4000 0.00025 0.03779996186494827 0.9858156028368794\n",
      "9 6000 0.00025 0.29004502296447754 0.9015151515151515\n",
      "9 8000 0.00025 0.038971319794654846 0.9797297297297297\n",
      "9 10000 0.00025 0.056995783001184464 0.9716312056737588\n",
      "9 12000 0.00025 0.036169301718473434 0.9931506849315068\n"
     ]
    }
   ],
   "source": [
    "#第13章/定义训练函数\n",
    "def train():\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=2e-3)\n",
    "    sched = torch.optim.lr_scheduler.StepLR(optim, step_size=3, gamma=0.5)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            #x = [8, 50]\n",
    "            #y = [8, 51]\n",
    "\n",
    "            #在训练时,是拿y的每一个字符输入,预测下一个字符,所以不需要最后一个字\n",
    "            #[8, 50, 39]\n",
    "            pred = model(x, y[:, :-1])\n",
    "\n",
    "            #[8, 50, 39] -> [400, 39]\n",
    "            pred = pred.reshape(-1, 39)\n",
    "\n",
    "            #[8, 51] -> [400]\n",
    "            y = y[:, 1:].reshape(-1)\n",
    "\n",
    "            #忽略pad\n",
    "            select = y != vocab_y['<PAD>']\n",
    "            pred = pred[select]\n",
    "            y = y[select]\n",
    "\n",
    "            loss = loss_func(pred, y)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            if i % 2000 == 0:\n",
    "                #[select, 39] -> [select]\n",
    "                pred = pred.argmax(1)\n",
    "                correct = (pred == y).sum().item()\n",
    "                accuracy = correct / len(pred)\n",
    "                lr = optim.param_groups[0]['lr']\n",
    "                print(epoch, i, lr, loss.item(), accuracy)\n",
    "\n",
    "        sched.step()\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5898f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "<SOS>69959127972a79068236990629479939<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "<SOS>79068237060588607911<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "<SOS>79068237060588607911<EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS>117886<EOS><EOS>091287778\n",
      "1\n",
      "<SOS>97585554595965a7879935899365<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "<SOS>105465490495330<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "<SOS>105465490495330<EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS>2666\n",
      "2\n",
      "<SOS>948296474994975a9947925666588821607<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "<SOS>9948873963063816582<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "<SOS>9948873963063816582<EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS>5<EOS><EOS><EOS><EOS><EOS>1336887\n",
      "3\n",
      "<SOS>81248996476a995886487475647426<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "<SOS>995886568724643902<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "<SOS>995886568724643902<EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS>7878\n",
      "4\n",
      "<SOS>419918877798a55730859672<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "<SOS>475649737470<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "<SOS>475649737470<EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS>\n",
      "5\n",
      "<SOS>498702999979a39598334951<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "<SOS>538301334930<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "<SOS>538301334920<EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS>1\n",
      "6\n",
      "<SOS>978948864349646794a762665688863796<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "<SOS>979711530038510590<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "<SOS>979711530038510590<EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS>670\n",
      "7\n",
      "<SOS>8388799497289839295a5994377949610297<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "<SOS>8394793875239449592<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "<SOS>8394793875239459592<EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS><EOS>19<EOS><EOS><EOS><EOS><EOS>4575555\n"
     ]
    }
   ],
   "source": [
    "#第13章/测试\n",
    "def test():\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        break\n",
    "\n",
    "    for i in range(8):\n",
    "        print(i)\n",
    "        print(''.join([vocab_xr[i] for i in x[i].tolist()]))\n",
    "        print(''.join([vocab_yr[i] for i in y[i].tolist()]))\n",
    "        print(''.join(\n",
    "            [vocab_yr[i] for i in predict(x[i].unsqueeze(0))[0].tolist()]))\n",
    "\n",
    "\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
