{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4238443f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(            token\n",
       " word             \n",
       " <PAD>           0\n",
       " <SOS>           1\n",
       " <EOS>           2\n",
       " <NUM>           3\n",
       " <UNK>           4\n",
       " ...           ...\n",
       " eastbound   14784\n",
       " clouds      14785\n",
       " repave      14786\n",
       " complained  14787\n",
       " dominate    14788\n",
       " \n",
       " [14789 rows x 1 columns],\n",
       "              word\n",
       " token            \n",
       " 0           <PAD>\n",
       " 1           <SOS>\n",
       " 2           <EOS>\n",
       " 3           <NUM>\n",
       " 4           <UNK>\n",
       " ...           ...\n",
       " 14784   eastbound\n",
       " 14785      clouds\n",
       " 14786      repave\n",
       " 14787  complained\n",
       " 14788    dominate\n",
       " \n",
       " [14789 rows x 1 columns])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第14章/读取字典\n",
    "import pandas as pd\n",
    "\n",
    "vocab = pd.read_csv('data/msr_paraphrase_vocab.csv', index_col='word')\n",
    "vocab_r = pd.read_csv('data/msr_paraphrase_vocab.csv', index_col='token')\n",
    "\n",
    "vocab, vocab_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66af0842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5801,\n",
       " same                                                        1\n",
       " s1_lens                                                    16\n",
       " s2_lens                                                    17\n",
       " pad_lens                                                   39\n",
       " sent        1,11,12,13,14,15,16,17,18,19,20,21,22,13,23,2,...\n",
       " Name: 0, dtype: object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第14章/定义数据集\n",
    "import torch\n",
    "\n",
    "\n",
    "class MsrDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        data = pd.read_csv('data/msr_paraphrase_data.csv')\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.data.iloc[i]\n",
    "\n",
    "\n",
    "dataset = MsrDataset()\n",
    "\n",
    "len(dataset), dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84198c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1640811701593/work/torch/csrc/utils/tensor_new.cpp:201.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1, 0]),\n",
       " tensor([[ 1, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 13, 23,  2, 24, 25,\n",
       "          26, 27, 28, 18, 19, 11, 12, 13, 14, 20, 21, 22, 13, 23,  2,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 1, 29, 30, 31, 32, 33, 34, 18, 35, 25, 36, 37,  3, 38,  3,  3, 39,  2,\n",
       "          29, 40, 31, 32, 37,  3, 38,  3, 41, 42, 43, 44, 25, 36, 38,  3,  3, 39,\n",
       "          37,  3,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "          2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
       "          2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第14章/定义数据整理函数\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    #取出数据\n",
    "    same = [i['same'] for i in data]\n",
    "    sent = [i['sent'] for i in data]\n",
    "    s1_lens = [i['s1_lens'] for i in data]\n",
    "    s2_lens = [i['s2_lens'] for i in data]\n",
    "    pad_lens = [i['pad_lens'] for i in data]\n",
    "\n",
    "    seg = []\n",
    "    for i in range(len(sent)):\n",
    "        #seg的形状和sent一样,但是内容不一样\n",
    "        #补PAD的位置是0,s1的位置是1,s2的位置是2\n",
    "        seg.append([1] * s1_lens[i] + [2] * s2_lens[i] + [0] * pad_lens[i])\n",
    "\n",
    "    #sent由字符型转换为list\n",
    "    sent = [np.array(i.split(','), dtype=np.int) for i in sent]\n",
    "\n",
    "    same = torch.LongTensor(same)\n",
    "    sent = torch.LongTensor(sent)\n",
    "    seg = torch.LongTensor(seg)\n",
    "\n",
    "    return same, sent, seg\n",
    "\n",
    "\n",
    "collate_fn([dataset[0], dataset[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc355fc4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第14章/定义数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=32,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True,\n",
    "                                     collate_fn=collate_fn)\n",
    "\n",
    "len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45f2de87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "         0, 1, 0, 1, 0, 1, 1, 1]),\n",
       " torch.Size([32, 72]),\n",
       " torch.Size([32, 72]),\n",
       " tensor([   1,  555, 1538, 1159,  720, 1405,  359, 6912,   18, 5386,   38, 1757,\n",
       "           42, 3992, 2125,    2, 1405, 6913,   65,  153, 1757,   42, 3992, 2125,\n",
       "          154, 6722,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第14章/查看数据样例\n",
    "for i, (same, sent, seg) in enumerate(loader):\n",
    "    break\n",
    "\n",
    "same, sent.shape, seg.shape, sent[0], seg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21ad1849",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    5,     5,     5,  8342,     5,  7568,  2984,     5,     5,     5,\n",
       "            5,     5,     5,     5,     5,     5,  7238,     5, 10831,     5,\n",
       "         5635,     5,     5,     5,    25,  6697,     5,     5,     5,     5,\n",
       "            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
       "            5,     5, 13424,     5,  3787,     5,     5,     5,  4711,     5,\n",
       "          448,   704,     5,  4711,  5834, 12720,     5,     5,     5,     5,\n",
       "           32,     5,     5,     5,     5,  1364,     5,   163,   238,     5,\n",
       "            5,     5,     5,   394,     5,    25,     5,     5,     5,     5,\n",
       "            5,     5,     5,     5, 13377,     5,     5,     5,     5,    18,\n",
       "            5,     5,     5,     5,     5,    69,     5,     5,     5,   145,\n",
       "            5,     5, 10282,     5,     5,     5,     5,     5,     5,     5,\n",
       "            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
       "            5,     5,     5,  1740,     5,     5,     5,     5,     5,     5,\n",
       "            5,     5,    65,     5,     5,     5,     5,     5,     5,     5,\n",
       "            5,     5,     5,     5,     5,     5,   950,     5,     5,     5,\n",
       "           18,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
       "        10081,     5,     5,     5,     5,     5,     5,     5,  2406,     5,\n",
       "            5,     5,     5,     5,  6908,     5,     5,     5,  3724,     5,\n",
       "            5,     5,     5,     5,     5,     5,     5,     5,   686,   631,\n",
       "            5,     5,     5,     5,     5,     5,     5,     5,  1536,     5,\n",
       "         1652,     5,    18,  7997])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第14章/定义随机替换函数\n",
    "import random\n",
    "\n",
    "\n",
    "def random_replace(sent):\n",
    "    #sent = [b, 72]\n",
    "    #不影响原来的sent\n",
    "    sent = sent.clone()\n",
    "\n",
    "    #替换矩阵,形状和sent一样,被替换过的位置是True,其他位置是False\n",
    "    replace = sent == -1\n",
    "\n",
    "    #遍历所有的字\n",
    "    for i in range(len(sent)):\n",
    "        for j in range(len(sent[i])):\n",
    "            #如果是符号就不操作了,只替换字\n",
    "            if sent[i, j] <= 10:\n",
    "                continue\n",
    "\n",
    "            #0.15的概率做操作\n",
    "            if random.random() > 0.15:\n",
    "                continue\n",
    "\n",
    "            #被操作过的位置标记下,这里的操作包括什么也不做\n",
    "            replace[i, j] = True\n",
    "\n",
    "            #分概率做不同的操作\n",
    "            p = random.random()\n",
    "\n",
    "            #0.8的概率替换为mask\n",
    "            if p < 0.8:\n",
    "                sent[i, j] = vocab.loc['<MASK>'].token\n",
    "\n",
    "            #0.1的概率不替换\n",
    "            elif p < 0.9:\n",
    "                pass\n",
    "\n",
    "            #0.1的概率替换成随机字\n",
    "            else:\n",
    "                #随机一个不是符号的字\n",
    "                rand_word = 0\n",
    "                while rand_word <= 10:\n",
    "                    rand_word = random.randint(0, len(vocab) - 1)\n",
    "                sent[i, j] = rand_word\n",
    "\n",
    "    return sent, replace\n",
    "\n",
    "\n",
    "replace_sent, replace = random_replace(sent)\n",
    "\n",
    "replace_sent[replace]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d23d512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 72]),\n",
       " torch.Size([72, 72]),\n",
       " tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True]),\n",
       " tensor([[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第14章/定义获取mask函数\n",
    "def get_mask(seg):\n",
    "    #key_padding_mask的定义方式为句子中是PAD的位置为True，否则是False\n",
    "    key_padding_mask = seg == 0\n",
    "\n",
    "    #在encode阶段不需要定义encode_attn_mask\n",
    "    #定义为None或者全False都可以\n",
    "    encode_attn_mask = torch.ones(72, 72) == -1\n",
    "\n",
    "    return key_padding_mask, encode_attn_mask\n",
    "\n",
    "\n",
    "key_padding_mask, encode_attn_mask = get_mask(seg)\n",
    "\n",
    "key_padding_mask.shape, encode_attn_mask.shape, key_padding_mask[\n",
    "    0], encode_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "569f02c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 2]), torch.Size([32, 72, 14789]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第14章/定义模型\n",
    "class BERTModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #定义词向量编码层\n",
    "        self.sent_embed = torch.nn.Embedding(num_embeddings=len(vocab),\n",
    "                                             embedding_dim=256)\n",
    "\n",
    "        #定义seg编码层\n",
    "        self.seg_embed = torch.nn.Embedding(num_embeddings=3,\n",
    "                                            embedding_dim=256)\n",
    "\n",
    "        #定义位置编码层\n",
    "        self.position_embed = torch.nn.Parameter(torch.randn(72, 256) / 10)\n",
    "\n",
    "        #定义编码层\n",
    "        encoder_layer = torch.nn.TransformerEncoderLayer(d_model=256,\n",
    "                                                         nhead=4,\n",
    "                                                         dim_feedforward=256,\n",
    "                                                         dropout=0.2,\n",
    "                                                         activation='relu',\n",
    "                                                         batch_first=True,\n",
    "                                                         norm_first=True)\n",
    "\n",
    "        #定义标准化层\n",
    "        norm = torch.nn.LayerNorm(normalized_shape=256,\n",
    "                                  elementwise_affine=True)\n",
    "\n",
    "        #定义编码器\n",
    "        self.encoder = torch.nn.TransformerEncoder(encoder_layer=encoder_layer,\n",
    "                                                   num_layers=3,\n",
    "                                                   norm=norm)\n",
    "\n",
    "        #定义same输出层\n",
    "        self.fc_same = torch.nn.Linear(in_features=256, out_features=2)\n",
    "\n",
    "        #定义sent输出层\n",
    "        self.fc_sent = torch.nn.Linear(in_features=256,\n",
    "                                       out_features=len(vocab))\n",
    "\n",
    "    def forward(self, sent, seg):\n",
    "        #sent -> [b, 72]\n",
    "        #seg -> [b, 72]\n",
    "\n",
    "        #获取mask\n",
    "        #[b, 72] -> [b, 72],[72, 72]\n",
    "        key_padding_mask, encode_attn_mask = get_mask(seg)\n",
    "\n",
    "        #编码,添加位置信息\n",
    "        #[b, 72] -> [b, 72, 256]\n",
    "        embed = self.sent_embed(sent) + self.seg_embed(\n",
    "            seg) + self.position_embed\n",
    "\n",
    "        #编码器计算\n",
    "        #[b, 72, 256] -> [b, 72, 256]\n",
    "        memory = self.encoder(src=embed,\n",
    "                              mask=encode_attn_mask,\n",
    "                              src_key_padding_mask=key_padding_mask)\n",
    "\n",
    "        #计算输出,same的输出使用第0个词的信息计算\n",
    "        #[b, 256] -> [b, 2]\n",
    "        same = self.fc_same(memory[:, 0])\n",
    "        #[b, 72, 256] -> [b, 72, V]\n",
    "        sent = self.fc_sent(memory)\n",
    "\n",
    "        return same, sent\n",
    "\n",
    "\n",
    "model = BERTModel()\n",
    "\n",
    "pred_same, pred_sent = model(sent, seg)\n",
    "\n",
    "pred_same.shape, pred_sent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b205cfe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 180 7.583975791931152 0.71875 0.05202312138728324\n",
      "5 180 7.01627254486084 0.71875 0.05357142857142857\n",
      "10 180 6.79768705368042 0.8125 0.08021390374331551\n",
      "15 180 6.868100166320801 0.71875 0.09947643979057591\n",
      "20 180 6.631269454956055 0.6875 0.09375\n",
      "25 180 6.179685115814209 0.6875 0.13043478260869565\n",
      "30 180 6.160305023193359 0.6875 0.10650887573964497\n",
      "35 180 6.393719673156738 0.6875 0.1187214611872146\n",
      "40 180 6.344337463378906 0.8125 0.11518324607329843\n",
      "45 180 5.619174957275391 0.65625 0.16304347826086957\n",
      "50 180 6.080831527709961 0.625 0.08791208791208792\n",
      "55 180 5.971240997314453 0.65625 0.10429447852760736\n",
      "60 180 5.649734973907471 0.78125 0.14285714285714285\n",
      "65 180 5.619324207305908 0.71875 0.14792899408284024\n",
      "70 180 5.730544090270996 0.78125 0.13131313131313133\n",
      "75 180 5.373783111572266 0.75 0.12206572769953052\n",
      "80 180 4.9394941329956055 0.84375 0.1686046511627907\n",
      "85 180 5.006924629211426 0.6875 0.1657754010695187\n",
      "90 180 4.841911792755127 0.78125 0.15463917525773196\n",
      "95 180 5.0255537033081055 0.65625 0.12837837837837837\n",
      "100 180 5.17122220993042 0.78125 0.13513513513513514\n",
      "105 180 4.811415672302246 0.75 0.1318181818181818\n",
      "110 180 4.607539176940918 0.78125 0.17045454545454544\n",
      "115 180 4.639654159545898 0.96875 0.17142857142857143\n",
      "120 180 4.500678062438965 0.8125 0.2028985507246377\n",
      "125 180 4.380643367767334 0.9375 0.1658291457286432\n",
      "130 180 4.384151935577393 0.84375 0.20422535211267606\n",
      "135 180 4.3378705978393555 0.75 0.15625\n",
      "140 180 4.341249465942383 0.875 0.2079207920792079\n",
      "145 180 4.139103412628174 0.875 0.1871345029239766\n",
      "150 180 4.3063201904296875 0.96875 0.19576719576719576\n",
      "155 180 4.398987770080566 0.875 0.19021739130434784\n",
      "160 180 4.082583904266357 0.875 0.2159090909090909\n",
      "165 180 4.157662391662598 0.9375 0.21256038647342995\n",
      "170 180 4.2265214920043945 0.90625 0.1797752808988764\n",
      "175 180 4.349081516265869 0.875 0.17045454545454544\n",
      "180 180 3.9014647006988525 0.90625 0.2342857142857143\n",
      "185 180 4.1119794845581055 0.9375 0.21022727272727273\n",
      "190 180 3.963508367538452 0.9375 0.22950819672131148\n",
      "195 180 3.910205364227295 0.96875 0.2111801242236025\n",
      "200 180 3.8339498043060303 0.875 0.22413793103448276\n",
      "205 180 4.186023235321045 0.9375 0.2125\n",
      "210 180 3.5579581260681152 0.84375 0.2459016393442623\n",
      "215 180 4.090705871582031 0.90625 0.20108695652173914\n",
      "220 180 3.8433139324188232 0.9375 0.20707070707070707\n",
      "225 180 3.7105391025543213 0.9375 0.2430939226519337\n",
      "230 180 3.532221794128418 1.0 0.28125\n",
      "235 180 3.7809479236602783 0.9375 0.29518072289156627\n",
      "240 180 3.676429510116577 0.90625 0.24630541871921183\n",
      "245 180 3.70375394821167 0.875 0.23121387283236994\n",
      "250 180 3.6268346309661865 0.90625 0.24210526315789474\n",
      "255 180 3.590047836303711 0.96875 0.3018867924528302\n",
      "260 180 3.909839153289795 0.96875 0.20093457943925233\n",
      "265 180 3.5165019035339355 0.96875 0.24598930481283424\n",
      "270 180 3.6305480003356934 0.875 0.26436781609195403\n",
      "275 180 3.2000343799591064 0.96875 0.29069767441860467\n",
      "280 180 3.561152458190918 0.90625 0.26108374384236455\n",
      "285 180 3.3436012268066406 0.875 0.35714285714285715\n",
      "290 180 3.04841947555542 0.9375 0.3333333333333333\n",
      "295 180 3.3482604026794434 0.96875 0.32515337423312884\n",
      "300 180 3.0588347911834717 0.96875 0.2751322751322751\n",
      "305 180 3.2204203605651855 0.90625 0.30687830687830686\n",
      "310 180 3.182868480682373 0.96875 0.30857142857142855\n",
      "315 180 3.350722551345825 0.96875 0.3054187192118227\n",
      "320 180 3.0649125576019287 0.96875 0.34831460674157305\n",
      "325 180 3.3560123443603516 1.0 0.31843575418994413\n",
      "330 180 3.36348557472229 0.96875 0.3\n",
      "335 180 3.0626118183135986 1.0 0.3241758241758242\n",
      "340 180 3.323854446411133 0.90625 0.2994350282485876\n",
      "345 180 3.0345962047576904 0.96875 0.3286384976525822\n",
      "350 180 3.0783309936523438 1.0 0.3202247191011236\n",
      "355 180 2.879584789276123 0.90625 0.3468208092485549\n",
      "360 180 3.0081090927124023 0.96875 0.3288590604026846\n",
      "365 180 3.1257071495056152 0.96875 0.3448275862068966\n",
      "370 180 3.1510989665985107 0.9375 0.35175879396984927\n",
      "375 180 2.7708966732025146 0.9375 0.37433155080213903\n",
      "380 180 2.644728660583496 0.96875 0.4326923076923077\n",
      "385 180 3.111387014389038 1.0 0.39622641509433965\n",
      "390 180 2.92587947845459 0.90625 0.34502923976608185\n",
      "395 180 2.861222505569458 0.9375 0.38953488372093026\n",
      "400 180 2.5893609523773193 1.0 0.4491017964071856\n",
      "405 180 2.8973381519317627 1.0 0.3655913978494624\n",
      "410 180 2.867147922515869 0.96875 0.39226519337016574\n",
      "415 180 2.9281513690948486 0.96875 0.3968253968253968\n",
      "420 180 2.703270196914673 0.9375 0.4029126213592233\n",
      "425 180 3.0893619060516357 0.9375 0.358695652173913\n",
      "430 180 2.828134059906006 1.0 0.4090909090909091\n",
      "435 180 2.5072286128997803 1.0 0.46236559139784944\n",
      "440 180 2.6934516429901123 0.9375 0.4187192118226601\n",
      "445 180 2.8843352794647217 0.875 0.39\n",
      "450 180 2.8292856216430664 1.0 0.4397905759162304\n",
      "455 180 2.9514389038085938 0.9375 0.39375\n",
      "460 180 2.92502498626709 0.9375 0.425\n",
      "465 180 2.734396457672119 0.96875 0.39593908629441626\n",
      "470 180 2.4567763805389404 0.9375 0.4479166666666667\n",
      "475 180 2.753169298171997 1.0 0.44041450777202074\n",
      "480 180 2.6687138080596924 0.96875 0.42162162162162165\n",
      "485 180 2.6386220455169678 1.0 0.43352601156069365\n",
      "490 180 2.385606527328491 1.0 0.43169398907103823\n",
      "495 180 2.4976978302001953 0.96875 0.44751381215469616\n",
      "500 180 2.439725875854492 0.90625 0.46411483253588515\n",
      "505 180 2.703981637954712 1.0 0.45614035087719296\n",
      "510 180 2.618845224380493 1.0 0.3988439306358382\n",
      "515 180 2.3957626819610596 1.0 0.437125748502994\n",
      "520 180 2.651198148727417 0.9375 0.4406779661016949\n",
      "525 180 2.6608774662017822 0.9375 0.40298507462686567\n",
      "530 180 2.6077487468719482 0.96875 0.44805194805194803\n",
      "535 180 2.508096218109131 0.96875 0.44559585492227977\n",
      "540 180 2.691983938217163 0.9375 0.441340782122905\n",
      "545 180 2.1169817447662354 0.96875 0.5111111111111111\n",
      "550 180 2.5088889598846436 0.96875 0.4473684210526316\n",
      "555 180 2.341325283050537 0.96875 0.4864864864864865\n",
      "560 180 2.4099602699279785 0.96875 0.5254237288135594\n",
      "565 180 2.2288877964019775 1.0 0.45989304812834225\n",
      "570 180 2.4328150749206543 0.96875 0.44385026737967914\n",
      "575 180 2.117417335510254 0.96875 0.49404761904761907\n",
      "580 180 2.3856704235076904 1.0 0.47643979057591623\n",
      "585 180 2.109222888946533 1.0 0.5647058823529412\n",
      "590 180 2.28188157081604 0.96875 0.44805194805194803\n",
      "595 180 2.4864821434020996 1.0 0.47770700636942676\n",
      "600 180 2.6923680305480957 1.0 0.436046511627907\n",
      "605 180 2.6283204555511475 1.0 0.40718562874251496\n",
      "610 180 2.0998642444610596 0.96875 0.5191256830601093\n",
      "615 180 2.115241050720215 1.0 0.5161290322580645\n",
      "620 180 2.0309979915618896 0.9375 0.5591397849462365\n",
      "625 180 2.2114315032958984 0.96875 0.4900990099009901\n",
      "630 180 1.8994648456573486 1.0 0.5401069518716578\n",
      "635 180 2.1612942218780518 1.0 0.5454545454545454\n",
      "640 180 2.3665430545806885 1.0 0.4896907216494845\n",
      "645 180 2.138890027999878 1.0 0.5248618784530387\n",
      "650 180 2.0328829288482666 0.90625 0.5392156862745098\n",
      "655 180 2.180661916732788 0.96875 0.5\n",
      "660 180 2.363367795944214 1.0 0.43820224719101125\n",
      "665 180 2.4131908416748047 0.96875 0.4685714285714286\n",
      "670 180 1.9668195247650146 0.9375 0.5722543352601156\n",
      "675 180 2.434281349182129 1.0 0.4817073170731707\n",
      "680 180 2.2819571495056152 0.96875 0.47150259067357514\n",
      "685 180 2.2626655101776123 1.0 0.5380952380952381\n",
      "690 180 2.0084872245788574 0.96875 0.5542857142857143\n",
      "695 180 2.3561854362487793 0.9375 0.5024630541871922\n",
      "700 180 2.3155455589294434 0.90625 0.4968944099378882\n",
      "705 180 1.950346827507019 1.0 0.5540540540540541\n",
      "710 180 1.8607592582702637 1.0 0.5591397849462365\n",
      "715 180 2.2333221435546875 0.9375 0.4624277456647399\n",
      "720 180 2.071898937225342 0.96875 0.5106382978723404\n",
      "725 180 2.1002047061920166 0.9375 0.5168539325842697\n",
      "730 180 2.1447854042053223 1.0 0.5154639175257731\n",
      "735 180 2.3569607734680176 0.96875 0.48066298342541436\n",
      "740 180 2.0434272289276123 0.96875 0.52\n",
      "745 180 2.263047456741333 1.0 0.5103092783505154\n",
      "750 180 1.9040175676345825 1.0 0.4864864864864865\n",
      "755 180 2.1387455463409424 1.0 0.536723163841808\n",
      "760 180 2.1724789142608643 1.0 0.49079754601226994\n",
      "765 180 1.8117341995239258 0.96875 0.5842105263157895\n",
      "770 180 2.159006118774414 0.96875 0.5178571428571429\n",
      "775 180 2.1962528228759766 1.0 0.5409836065573771\n",
      "780 180 2.1397793292999268 0.96875 0.521978021978022\n",
      "785 180 1.9755196571350098 1.0 0.5301204819277109\n",
      "790 180 1.8843222856521606 0.96875 0.609375\n",
      "795 180 1.8056086301803589 1.0 0.5555555555555556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 180 1.6947892904281616 1.0 0.5945945945945946\n",
      "805 180 2.1447057723999023 1.0 0.5245098039215687\n",
      "810 180 2.0740888118743896 1.0 0.5444444444444444\n",
      "815 180 1.9558393955230713 0.96875 0.5277777777777778\n",
      "820 180 2.089111804962158 0.96875 0.5536723163841808\n",
      "825 180 1.8715225458145142 0.96875 0.6043956043956044\n",
      "830 180 1.7022827863693237 0.9375 0.5632183908045977\n",
      "835 180 2.0040524005889893 0.9375 0.5583756345177665\n",
      "840 180 2.0123708248138428 0.96875 0.5459770114942529\n",
      "845 180 1.785885214805603 0.96875 0.5661375661375662\n",
      "850 180 1.9366830587387085 0.9375 0.5577889447236181\n",
      "855 180 2.1804163455963135 0.96875 0.5195530726256983\n",
      "860 180 1.804997444152832 0.9375 0.5751295336787565\n",
      "865 180 1.9716240167617798 0.96875 0.5463917525773195\n",
      "870 180 1.7420647144317627 0.96875 0.6388888888888888\n",
      "875 180 2.02553391456604 0.90625 0.5153061224489796\n",
      "880 180 1.9450682401657104 0.9375 0.6162790697674418\n",
      "885 180 1.7774468660354614 0.96875 0.5860215053763441\n",
      "890 180 1.6604479551315308 1.0 0.625\n",
      "895 180 1.7253432273864746 0.96875 0.6111111111111112\n",
      "900 180 1.7595065832138062 1.0 0.6217616580310881\n",
      "905 180 1.6868664026260376 1.0 0.5802469135802469\n",
      "910 180 1.5835188627243042 1.0 0.6352941176470588\n",
      "915 180 1.947129487991333 1.0 0.5588235294117647\n",
      "920 180 1.9735697507858276 1.0 0.5773195876288659\n",
      "925 180 1.891697645187378 0.96875 0.5144508670520231\n",
      "930 180 1.7385632991790771 0.9375 0.5964912280701754\n",
      "935 180 2.0006651878356934 0.9375 0.5472636815920398\n",
      "940 180 2.11030650138855 0.9375 0.5577889447236181\n",
      "945 180 2.086975574493408 1.0 0.5544554455445545\n",
      "950 180 1.9702913761138916 0.96875 0.5274725274725275\n",
      "955 180 1.9379172325134277 1.0 0.5097087378640777\n",
      "960 180 1.8716912269592285 0.96875 0.5505050505050505\n",
      "965 180 1.7746073007583618 1.0 0.6033519553072626\n",
      "970 180 2.0395607948303223 0.9375 0.5208333333333334\n",
      "975 180 2.1404318809509277 1.0 0.5076142131979695\n",
      "980 180 1.7581708431243896 1.0 0.5918367346938775\n",
      "985 180 1.6094070672988892 1.0 0.6206896551724138\n",
      "990 180 1.748611330986023 1.0 0.5988023952095808\n",
      "995 180 1.6734157800674438 0.9375 0.6103896103896104\n",
      "1000 180 1.832932949066162 0.9375 0.5780346820809249\n",
      "1005 180 1.7954154014587402 1.0 0.6181818181818182\n",
      "1010 180 1.51235830783844 1.0 0.6010928961748634\n",
      "1015 180 1.5483417510986328 1.0 0.6529411764705882\n",
      "1020 180 1.5095406770706177 0.96875 0.6685714285714286\n",
      "1025 180 1.619802474975586 0.96875 0.6146341463414634\n",
      "1030 180 1.8626765012741089 0.96875 0.5974842767295597\n",
      "1035 180 1.6126224994659424 0.96875 0.625\n",
      "1040 180 1.355442762374878 1.0 0.6538461538461539\n",
      "1045 180 1.3940402269363403 0.90625 0.6717171717171717\n",
      "1050 180 1.6819907426834106 0.9375 0.5380116959064327\n",
      "1055 180 1.7777632474899292 0.9375 0.5792349726775956\n",
      "1060 180 1.685197114944458 0.96875 0.6256410256410256\n",
      "1065 180 1.6617200374603271 0.9375 0.5932203389830508\n",
      "1070 180 1.5833157300949097 1.0 0.6073619631901841\n",
      "1075 180 2.1086676120758057 1.0 0.4891304347826087\n",
      "1080 180 1.6987907886505127 0.96875 0.6263157894736842\n",
      "1085 180 1.3420498371124268 0.84375 0.6630434782608695\n",
      "1090 180 1.341647744178772 1.0 0.6909090909090909\n",
      "1095 180 1.4389406442642212 1.0 0.6646341463414634\n",
      "1100 180 1.8353255987167358 0.96875 0.5706214689265536\n",
      "1105 180 1.8087573051452637 0.96875 0.5405405405405406\n",
      "1110 180 1.527998447418213 0.96875 0.6024096385542169\n",
      "1115 180 1.3662852048873901 0.96875 0.6529411764705882\n",
      "1120 180 1.651123046875 0.96875 0.6063829787234043\n",
      "1125 180 1.7205854654312134 0.9375 0.5975609756097561\n",
      "1130 180 1.4852582216262817 1.0 0.6324324324324324\n",
      "1135 180 1.724000096321106 0.9375 0.56\n",
      "1140 180 1.5625075101852417 0.9375 0.6135265700483091\n",
      "1145 180 1.3643085956573486 0.96875 0.6951219512195121\n",
      "1150 180 1.741936445236206 1.0 0.5732484076433121\n",
      "1155 180 1.6204837560653687 0.96875 0.612565445026178\n",
      "1160 180 1.636760950088501 1.0 0.6313131313131313\n",
      "1165 180 1.6537481546401978 1.0 0.5549132947976878\n",
      "1170 180 1.532002568244934 0.96875 0.6525821596244131\n",
      "1175 180 1.4277838468551636 0.96875 0.6868686868686869\n",
      "1180 180 1.4611623287200928 1.0 0.6604938271604939\n",
      "1185 180 1.4608763456344604 1.0 0.6243654822335025\n",
      "1190 180 1.3018966913223267 1.0 0.689119170984456\n",
      "1195 180 1.71296226978302 0.96875 0.6074766355140186\n",
      "1200 180 1.2390588521957397 1.0 0.7010869565217391\n",
      "1205 180 1.4357632398605347 1.0 0.647887323943662\n",
      "1210 180 1.9237773418426514 1.0 0.569060773480663\n",
      "1215 180 1.3680839538574219 0.96875 0.6845637583892618\n",
      "1220 180 1.3901222944259644 1.0 0.6722222222222223\n",
      "1225 180 1.3154278993606567 0.9375 0.6073619631901841\n",
      "1230 180 1.2156513929367065 0.96875 0.6704545454545454\n",
      "1235 180 1.8337441682815552 1.0 0.5916230366492147\n",
      "1240 180 1.397815465927124 0.96875 0.6402116402116402\n",
      "1245 180 1.5005383491516113 0.96875 0.6296296296296297\n",
      "1250 180 1.2880150079727173 1.0 0.6740331491712708\n",
      "1255 180 1.3407598733901978 1.0 0.6802325581395349\n",
      "1260 180 1.3971682786941528 0.90625 0.6446700507614214\n",
      "1265 180 1.2924984693527222 1.0 0.6878612716763006\n",
      "1270 180 1.2794325351715088 0.9375 0.6486486486486487\n",
      "1275 180 1.3420695066452026 0.9375 0.6827956989247311\n",
      "1280 180 1.4303175210952759 1.0 0.6342857142857142\n",
      "1285 180 1.3796194791793823 1.0 0.651685393258427\n",
      "1290 180 1.3321853876113892 0.96875 0.66875\n",
      "1295 180 1.2744320631027222 0.96875 0.6612903225806451\n",
      "1300 180 1.2721999883651733 0.96875 0.675392670157068\n",
      "1305 180 1.362457036972046 1.0 0.7010869565217391\n",
      "1310 180 1.086226224899292 0.96875 0.7485380116959064\n",
      "1315 180 1.2234389781951904 1.0 0.6923076923076923\n",
      "1320 180 1.5887970924377441 1.0 0.6309523809523809\n",
      "1325 180 1.2825149297714233 1.0 0.6647058823529411\n",
      "1330 180 1.4668903350830078 0.90625 0.6851851851851852\n",
      "1335 180 1.1797529458999634 1.0 0.7258064516129032\n",
      "1340 180 1.4873523712158203 1.0 0.6290322580645161\n",
      "1345 180 1.4256553649902344 1.0 0.6650717703349283\n",
      "1350 180 1.337387204170227 1.0 0.6865671641791045\n",
      "1355 180 1.2281289100646973 0.96875 0.703030303030303\n",
      "1360 180 1.1452275514602661 0.9375 0.7262569832402235\n",
      "1365 180 1.1469614505767822 0.96875 0.6842105263157895\n",
      "1370 180 1.13857102394104 1.0 0.7352941176470589\n",
      "1375 180 1.0175493955612183 1.0 0.735\n",
      "1380 180 1.191727876663208 0.96875 0.7371428571428571\n",
      "1385 180 1.4278632402420044 1.0 0.6863905325443787\n",
      "1390 180 1.3749204874038696 0.9375 0.7010309278350515\n",
      "1395 180 1.2480041980743408 1.0 0.6855345911949685\n",
      "1400 180 1.1139582395553589 1.0 0.6914285714285714\n",
      "1405 180 1.2528616189956665 0.9375 0.7012987012987013\n",
      "1410 180 1.5384540557861328 0.9375 0.6555555555555556\n",
      "1415 180 0.9793370962142944 0.90625 0.7688172043010753\n",
      "1420 180 1.6453784704208374 0.96875 0.609375\n",
      "1425 180 1.2045304775238037 0.96875 0.7128205128205128\n",
      "1430 180 1.1440738439559937 1.0 0.7245508982035929\n",
      "1435 180 1.2728668451309204 0.96875 0.675\n",
      "1440 180 1.3193471431732178 0.96875 0.6666666666666666\n",
      "1445 180 1.016741394996643 0.96875 0.7513227513227513\n",
      "1450 180 1.233947515487671 0.96875 0.680628272251309\n",
      "1455 180 1.3125407695770264 0.96875 0.6287425149700598\n",
      "1460 180 1.15394926071167 1.0 0.7055555555555556\n",
      "1465 180 1.1809526681900024 0.96875 0.7225130890052356\n",
      "1470 180 1.2376956939697266 0.96875 0.7075471698113207\n",
      "1475 180 1.3579111099243164 1.0 0.6861702127659575\n",
      "1480 180 1.4489058256149292 1.0 0.6536312849162011\n",
      "1485 180 1.0481743812561035 1.0 0.7407407407407407\n",
      "1490 180 1.2166748046875 1.0 0.6625\n",
      "1495 180 1.317618727684021 0.96875 0.6789473684210526\n",
      "1500 180 1.0848037004470825 0.96875 0.7731958762886598\n",
      "1505 180 1.1598539352416992 0.96875 0.7288135593220338\n",
      "1510 180 1.10801100730896 1.0 0.740506329113924\n",
      "1515 180 1.2197386026382446 0.96875 0.6990291262135923\n",
      "1520 180 1.0567737817764282 0.96875 0.7191011235955056\n",
      "1525 180 1.2029153108596802 1.0 0.7189189189189189\n",
      "1530 180 1.244845986366272 1.0 0.6666666666666666\n",
      "1535 180 1.1672195196151733 1.0 0.7257142857142858\n",
      "1540 180 1.049559473991394 1.0 0.7544910179640718\n",
      "1545 180 1.5798250436782837 1.0 0.64\n",
      "1550 180 1.3091994524002075 0.96875 0.6585365853658537\n",
      "1555 180 0.9354751706123352 0.96875 0.7539267015706806\n",
      "1560 180 1.1219435930252075 0.96875 0.7225806451612903\n",
      "1565 180 1.2083040475845337 0.9375 0.6910112359550562\n",
      "1570 180 1.0656766891479492 1.0 0.725609756097561\n",
      "1575 180 0.8476354479789734 0.9375 0.7664670658682635\n",
      "1580 180 1.1788486242294312 1.0 0.7069767441860465\n",
      "1585 180 1.3006260395050049 0.9375 0.6904761904761905\n",
      "1590 180 1.1812682151794434 0.9375 0.7344632768361582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1595 180 1.5449641942977905 0.96875 0.6453488372093024\n",
      "1600 180 1.120186448097229 1.0 0.7103825136612022\n",
      "1605 180 0.812895655632019 0.96875 0.8\n",
      "1610 180 1.009718418121338 0.96875 0.7417582417582418\n",
      "1615 180 1.2270549535751343 0.96875 0.6779661016949152\n",
      "1620 180 1.0853394269943237 0.96875 0.7252747252747253\n",
      "1625 180 0.9983455538749695 1.0 0.7543859649122807\n",
      "1630 180 0.8749478459358215 0.96875 0.7514792899408284\n",
      "1635 180 0.9588981866836548 0.9375 0.7441860465116279\n",
      "1640 180 1.3310329914093018 0.96875 0.6666666666666666\n",
      "1645 180 0.860778272151947 1.0 0.7633136094674556\n",
      "1650 180 1.118299126625061 1.0 0.6818181818181818\n",
      "1655 180 0.8316289782524109 0.96875 0.7784090909090909\n",
      "1660 180 0.8396706581115723 0.96875 0.8128654970760234\n",
      "1665 180 1.3668744564056396 0.96875 0.6793478260869565\n",
      "1670 180 0.9910237789154053 0.9375 0.7430555555555556\n",
      "1675 180 1.167981743812561 0.96875 0.7344632768361582\n",
      "1680 180 0.8949767351150513 0.96875 0.7474226804123711\n",
      "1685 180 1.0360218286514282 1.0 0.7198067632850241\n",
      "1690 180 0.9856430292129517 1.0 0.7634408602150538\n",
      "1695 180 0.70965176820755 1.0 0.8064516129032258\n",
      "1700 180 0.9337102174758911 1.0 0.7486631016042781\n",
      "1705 180 1.4284772872924805 0.96875 0.6593406593406593\n",
      "1710 180 1.36286199092865 0.96875 0.6532663316582915\n",
      "1715 180 1.178903341293335 0.9375 0.6961325966850829\n",
      "1720 180 0.8528006076812744 1.0 0.7582417582417582\n",
      "1725 180 0.9602735042572021 0.96875 0.776536312849162\n",
      "1730 180 0.8353853821754456 1.0 0.7909604519774012\n",
      "1735 180 1.0235549211502075 0.96875 0.7189189189189189\n",
      "1740 180 0.8275982737541199 1.0 0.7875647668393783\n",
      "1745 180 0.7940751314163208 0.9375 0.848314606741573\n",
      "1750 180 1.2356350421905518 1.0 0.6702702702702703\n",
      "1755 180 1.0143065452575684 1.0 0.7225433526011561\n",
      "1760 180 1.020812749862671 0.96875 0.7643979057591623\n",
      "1765 180 1.1199060678482056 0.96875 0.7294117647058823\n",
      "1770 180 1.046350121498108 1.0 0.7157360406091371\n",
      "1775 180 1.035731554031372 0.96875 0.7159763313609467\n",
      "1780 180 0.8442676663398743 1.0 0.8181818181818182\n",
      "1785 180 0.8569318056106567 0.96875 0.7469879518072289\n",
      "1790 180 0.9588266611099243 1.0 0.7752808988764045\n",
      "1795 180 0.9902370572090149 0.96875 0.7426900584795322\n",
      "1800 180 0.9209166765213013 0.96875 0.7083333333333334\n",
      "1805 180 0.8445142507553101 0.875 0.803030303030303\n",
      "1810 180 0.897270917892456 0.96875 0.8\n",
      "1815 180 0.911293089389801 1.0 0.7579617834394905\n",
      "1820 180 0.7860994935035706 0.96875 0.7729468599033816\n",
      "1825 180 0.8049724698066711 1.0 0.8067632850241546\n",
      "1830 180 0.994225025177002 1.0 0.7514450867052023\n",
      "1835 180 0.9947243928909302 1.0 0.7365269461077845\n",
      "1840 180 1.086483120918274 1.0 0.7100591715976331\n",
      "1845 180 1.032315969467163 1.0 0.6908212560386473\n",
      "1850 180 1.008675456047058 1.0 0.7440476190476191\n",
      "1855 180 1.006692886352539 0.9375 0.7202380952380952\n",
      "1860 180 0.7462138533592224 0.96875 0.7931034482758621\n",
      "1865 180 0.9505536556243896 0.96875 0.7251461988304093\n",
      "1870 180 0.7898674011230469 1.0 0.8088235294117647\n",
      "1875 180 1.04042649269104 0.96875 0.7365591397849462\n",
      "1880 180 1.2254635095596313 0.96875 0.7386363636363636\n",
      "1885 180 1.0327545404434204 1.0 0.7715736040609137\n",
      "1890 180 0.8390346765518188 0.9375 0.770949720670391\n",
      "1895 180 1.2662718296051025 0.96875 0.6959459459459459\n",
      "1900 180 1.0864462852478027 1.0 0.6951219512195121\n",
      "1905 180 1.0304795503616333 0.96875 0.7539267015706806\n",
      "1910 180 0.8592067956924438 0.96875 0.7650273224043715\n",
      "1915 180 1.10627281665802 1.0 0.7142857142857143\n",
      "1920 180 0.7305423021316528 0.96875 0.7923497267759563\n",
      "1925 180 1.101048231124878 1.0 0.7421052631578947\n",
      "1930 180 0.8727186918258667 1.0 0.7631578947368421\n",
      "1935 180 1.2466249465942383 1.0 0.65625\n",
      "1940 180 1.195167064666748 0.96875 0.7632850241545893\n",
      "1945 180 0.8503151535987854 1.0 0.7471910112359551\n",
      "1950 180 0.9282615780830383 1.0 0.7637362637362637\n",
      "1955 180 0.8328812718391418 1.0 0.7860696517412935\n",
      "1960 180 0.9886853694915771 1.0 0.7591623036649214\n",
      "1965 180 0.9769376516342163 0.96875 0.7409326424870466\n",
      "1970 180 0.8885232210159302 0.9375 0.7663043478260869\n",
      "1975 180 0.8686213493347168 1.0 0.7857142857142857\n",
      "1980 180 0.859603226184845 1.0 0.795\n",
      "1985 180 0.9902157783508301 1.0 0.7395833333333334\n",
      "1990 180 0.8147439360618591 0.96875 0.7878787878787878\n",
      "1995 180 0.9632631540298462 1.0 0.7540106951871658\n"
     ]
    }
   ],
   "source": [
    "#第14章/训练\n",
    "def train():\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    for epoch in range(2000):\n",
    "        for i, (same, sent, seg) in enumerate(loader):\n",
    "            #same = [b]\n",
    "            #sent = [b, 72]\n",
    "            #seg = [b, 72]\n",
    "\n",
    "            #随机替换x中的某些字符,replace为是否被操作过的矩阵,这里的操作包括不替换\n",
    "            #replace_sent = [b, 72]\n",
    "            #replace = [b, 72]\n",
    "            replace_sent, replace = random_replace(sent)\n",
    "\n",
    "            #模型计算\n",
    "            #[b, 72],[b, 72] -> [b, 2],[b, 72, V]\n",
    "            pred_same, pred_sent = model(replace_sent, seg)\n",
    "\n",
    "            #pred_sent = pred_sent.flatten(end_dim=1)\n",
    "            #sent = sent.flatten()\n",
    "\n",
    "            #只把被操作过的字提取出来\n",
    "            #[b, 72, V] -> [replace, V]\n",
    "            pred_sent = pred_sent[replace]\n",
    "\n",
    "            #把被操作之前的字取出来\n",
    "            #[b, 72] -> [replace]\n",
    "            sent = sent[replace]\n",
    "\n",
    "            #计算两份loss,再加权求和\n",
    "            loss_same = loss_func(pred_same, same)\n",
    "            loss_sent = loss_func(pred_sent, sent)\n",
    "            loss = loss_same * 0.1 + loss_sent\n",
    "\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            #计算same预测正确率\n",
    "            pred_same = pred_same.argmax(dim=1)\n",
    "            acc_same = (same == pred_same).sum().item() / len(same)\n",
    "\n",
    "            #计算替换词预测正确率\n",
    "            pred_sent = pred_sent.argmax(dim=1)\n",
    "            acc_sent = (sent == pred_sent).sum().item() / len(sent)\n",
    "\n",
    "            print(epoch, i, loss.item(), acc_same, acc_sent)\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46c86c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<SOS> among three major candidates schwarzenegger is wining the battle for independents and crossover voters <EOS> schwarzenegger picks up more independents and crossover voters than bustamante <EOS>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第14章/定义工具函数，tensor转换为字符串\n",
    "def tensor_to_str(tensor):\n",
    "    #转换为list格式\n",
    "    tensor = tensor.tolist()\n",
    "    #过滤掉PAD\n",
    "    tensor = [i for i in tensor if i != vocab.loc['<PAD>'].token]\n",
    "    #转换为词\n",
    "    tensor = [vocab_r.loc[i].word for i in tensor]\n",
    "    #转换为字符串\n",
    "    return ' '.join(tensor)\n",
    "\n",
    "\n",
    "tensor_to_str(sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b589fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same= 0 pred_same= 1\n",
      "\n",
      "replace_sent= <SOS> among three major candidates schwarzenegger is wining the battle for independents and crossover <MASK> <EOS> schwarzenegger picks up more independents <MASK> crossover <MASK> than bustamante <EOS>\n",
      "\n",
      "sent= voters and voters\n",
      "\n",
      "pred_sent= before hanging distorting\n",
      "\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#第14章/定义工具函数，打印预测结果\n",
    "def print_predict(same, pred_same, replace_sent, sent, pred_sent, replace):\n",
    "    #输出same预测结果\n",
    "    same = same[0].item()\n",
    "    pred_same = pred_same.argmax(dim=1)[0].item()\n",
    "    print('same=', same, 'pred_same=', pred_same)\n",
    "    print()\n",
    "\n",
    "    #输出句子替换词的预测结果\n",
    "    replace_sent = tensor_to_str(replace_sent[0])\n",
    "    sent = tensor_to_str(sent[0][replace[0]])\n",
    "    pred_sent = tensor_to_str(pred_sent.argmax(dim=2)[0][replace[0]])\n",
    "    print('replace_sent=', replace_sent)\n",
    "    print()\n",
    "    print('sent=', sent)\n",
    "    print()\n",
    "    print('pred_sent=', pred_sent)\n",
    "    print()\n",
    "    print('-------------------------------------')\n",
    "\n",
    "\n",
    "print_predict(same, torch.randn(32, 2), replace_sent, sent,\n",
    "              torch.randn(32, 72, 100), replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ff8b9ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same= 1 pred_same= 1\n",
      "\n",
      "replace_sent= <SOS> this individual s lawyers are trying <MASK> obtain from the court a free pass to download or upload music online illegally <EOS> her lawyers are trying to obtain a <MASK> pass <MASK> <MASK> <MASK> upload <MASK> <MASK> line illegally <EOS>\n",
      "\n",
      "sent= to free to download or upload music on\n",
      "\n",
      "pred_sent= to free or download or upload music or\n",
      "\n",
      "-------------------------------------\n",
      "same= 1 pred_same= 1\n",
      "\n",
      "replace_sent= <SOS> a federal <MASK> court yesterday reinstated <MASK> charges against a san diego student accused of lying about his association <MASK> <NUM> <NUM> hijackers <EOS> a u s appeals court in <MASK> york <MASK> perjury charges against a grossmont college student accused of lying about his knowledge of two of the sept <NUM> hijackers <EOS>\n",
      "\n",
      "sent= appeals perjury his with new reinstated knowledge hijackers\n",
      "\n",
      "pred_sent= appeals perjury his with new reinstated knowledge hijackers\n",
      "\n",
      "-------------------------------------\n",
      "same= 0 pred_same= 0\n",
      "\n",
      "replace_sent= <SOS> he said <MASK> president bush s <MASK> clean <MASK> act amendment called the <MASK> skies initiative would result in miami efficiency and therefore less pollution <EOS> he said that <MASK> allowing power companies more flexibility the <MASK> <MASK> initiative would result in greater <MASK> and therefore <MASK> pollution <EOS>\n",
      "\n",
      "sent= that proposed air clear greater by clear skies efficiency less\n",
      "\n",
      "pred_sent= that proposed power result greater president result skies efficiency less\n",
      "\n",
      "-------------------------------------\n",
      "same= 0 pred_same= 0\n",
      "\n",
      "replace_sent= <SOS> currently <MASK> state s congressional delegation is made up of <NUM> democrats and <NUM> republicans <EOS> <MASK> used now hold every <MASK> office the state s <MASK> delegation comprises <NUM> democrats and <NUM> republicans <EOS>\n",
      "\n",
      "sent= the although republicans statewide congressional\n",
      "\n",
      "pred_sent= the republicans republicans statewide congressional\n",
      "\n",
      "-------------------------------------\n",
      "same= 1 pred_same= 1\n",
      "\n",
      "replace_sent= <SOS> the survey medicine found that executives <MASK> feel that current economic conditions have improved rose to <NUM> per cent from <NUM> per cent last quarter <EOS> the survey also found that more executives feel that current economic conditions have improved at <NUM> per <MASK> compared to <NUM> per cent in the <MASK> quarter <EOS>\n",
      "\n",
      "sent= also who to cent first\n",
      "\n",
      "pred_sent= were who to cent first\n",
      "\n",
      "-------------------------------------\n",
      "1.0\n",
      "0.87995337995338\n"
     ]
    }
   ],
   "source": [
    "#第14章/测试\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct_same = 0\n",
    "    total_same = 0\n",
    "    correct_sent = 0\n",
    "    total_sent = 0\n",
    "    for i, (same, sent, seg) in enumerate(loader):\n",
    "        #测试5个批次\n",
    "        if i == 5:\n",
    "            break\n",
    "        #same = [b]\n",
    "        #sent = [b, 72]\n",
    "        #seg = [b, 72]\n",
    "\n",
    "        #随机替换x中的某些字符,replace为是否被操作过的矩阵,这里的操作包括不替换\n",
    "        #replace_sent = [b, 72]\n",
    "        #replace = [b, 72]\n",
    "        replace_sent, replace = random_replace(sent)\n",
    "\n",
    "        #模型计算\n",
    "        #[b, 72],[b, 72] -> [b, 2],[b, 72, V]\n",
    "        with torch.no_grad():\n",
    "            pred_same, pred_sent = model(replace_sent, seg)\n",
    "\n",
    "        #输出预测结果\n",
    "        print_predict(same, pred_same, replace_sent, sent, pred_sent, replace)\n",
    "\n",
    "        #只把被操作过的字提取出来\n",
    "        #[b, 72, V] -> [replace, V]\n",
    "        pred_sent = pred_sent[replace]\n",
    "\n",
    "        #把被操作之前的字取出来\n",
    "        #[b, 72] -> [replace]\n",
    "        sent = sent[replace]\n",
    "\n",
    "        #计算same预测正确率\n",
    "        pred_same = pred_same.argmax(dim=1)\n",
    "        correct_same += (same == pred_same).sum().item()\n",
    "        total_same += len(same)\n",
    "\n",
    "        #计算替换词预测正确率\n",
    "        pred_sent = pred_sent.argmax(dim=1)\n",
    "        correct_sent += (sent == pred_sent).sum().item()\n",
    "        total_sent += len(sent)\n",
    "\n",
    "    print(correct_same / total_same)\n",
    "    print(correct_sent / total_sent)\n",
    "\n",
    "\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
